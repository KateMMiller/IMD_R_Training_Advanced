---
author: "John Paul Schmit & Thomas Parr"
date: "1/20/2022"
output: 
  html_document:
  css: custom_styles.css
---

#### Performance
```{r_Day2_Mod3_librarys}
library(tidyverse)
library(lubridate)
library(microbenchmark)
library(profvis)
```

  
<details open><summary class='drop'>Introduction</summary>

"Performance" here refers to how fast your code does something.

Faster is generally better, but it often requires more time to write and can include code that is harder to read.

Most of the time you can ignore performance. If you run the code and are happy with how long it takes, then there is no problem.

Performance can be a consideration when:

  1) You are doing the same analysis many (dozens to hundreds) of times. Think of an analysis of each species at many parks, or each water quality measure at many sites.  
    
  2) If you have very large data files that you need to manipulate.  
    
  3) If you are doing a simulation study and need to create and manipulate large numbers of data sets. 
    
  4) If you are doing some sort of interactive visualization and need results very quickly. 
  
  
For this section we will need the `tidyverse`, `lubridate`, `profvis` and `microbenchmark` libraries. 

```{r libraries_performance}

library(tidyverse)
library(lubridate)
library(profvis)
library(microbenchmark)
```

  
</details>

<br>

<details open><summary class='drop'> Vectorization </summary>
R has "vectorization" - this means there are some functions that take a vector as an input, perform a calculation or operation on each element and return a vector as output. Some examples:

Numeric Vector
```{r num_vector}
# A vector of numbers
x <- 1:10

# Arithmetic is vectorized
x + 1
```

Character Vector
```{r character_vector}
# a character vector (letters)

x <- letters[1:10]

# paste is vectorized
paste("Letter:", x)

```

Logical Vector
```{r logical_vector}
# a logical vector

x <- c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE)

# logical operations are vectorized

TRUE & x

```

Date Vector
```{r date_vector}
# a date vector

library(lubridate)

x <- mdy("02/07/2022", "02/08/2022", "02/09/2022", "02/10/2022")
x
```


```{r date_vector_add}
## adding 1 shows you the next day
x + 1

rm(x)

```

Most functions in R are vectorized. This includes:

1) Mathematical functions, including statistical functions.  
  
2) Logical operators.  
  
3) Functions such as `rowMeans()` that work on matrices.  
  
4) Text manipulation functions.  
  
Many of these functions are written in C which makes them extremely fast. 

You SHOULD NOT write a function or a `for()` loop to do iteration when the pre-existing vectorization will do the same thing.

You SHOULD make use of vectorization in your functions to keep them fast and readable.

When you use the `dplyr` function `mutate()` you are making use of vectorization. 

</details>


<details open><summary class='drop'>Profiling</summary>

"Profiling" is looking at each part of a function to find out how long it takes. This can allow you to identify bottlenecks where speeding up your code might be helpful. Then tools such as `microbenchmark` can be used to compare options. RStudio has profiling tools built in but to use then you need to install the `profvis` package.

Once you do this you can access the Profile menu in the top menu bar. You can select some code, and use "Profile Selected Line(s)" to run the profiler.

![RStudio](images/profile_start.jpg)

However, you can run the `profvis()` function to do the same thing with a little more control over the outcome. Here we will run the function. In case you no longer have the logger data handy - here is the code to load it again.

```{r prof_vis_load_data, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(profvis)

# Recreate intensity data
fNames <- c(
  "APIS01_20548905_2021_temp.csv",
  "APIS02_20549198_2021_temp.csv",
  "APIS03_20557246_2021_temp.csv",
  "APIS04_20597702_2021_temp.csv",
  "APIS05_20597703_2021_temp.csv"
)

fPaths <- paste0("https://raw.githubusercontent.com/KateMMiller/IMD_R_Training_Intro/master/Data/", fNames)

intensity_data_raw <- set_names(fPaths, c("APIS01", "APIS02", "APIS03", "APIS04", "APIS05")) %>%
  map(~ read_csv(file = .x, , skip = 1)) %>%
  discard(~ nrow(.x) == 0)


# Fix_Data function
Fix_Data <- function(data) {
  data %>%
    select(starts_with("Date") | starts_with("Temp") | starts_with("Intensity")) %>%
    rename("Date_Time" = starts_with("Date"), "Temp_F" = starts_with("Temp"), "Intensity" = starts_with("Intensity")) %>%
    mutate(Temp_C = 5 * (Temp_F - 32) / 9, Date_Time = mdy_hms(Date_Time), Date = date(Date_Time))
}


```

With that loaded we can run `profvis()`

```{r profvis_run, eval=FALSE}

profvis::profvis(Fix_Data(intensity_data_raw[[2]]), interval = 0.005)

```


This will give you a "Flame Graph" which tells you how much time each part of your function was running. Here we will profile the `Fix_Data()` function from earlier. 

![RStudio](images/flame_graph.jpg)

The graph shows time across the bottom. Looking from the bottom to the top you see how long various parts took. You can see that `profvis` (the profile tool) in the bottom row was running the whole time. One row up is `rename()` from `dplyr`a nd `mdy_hms()` from `lubridate`. Only 5 ms was spent on `rename()` where as 15ms was spent on `mdy_hms()`. Abover those two function are the fucntions that they called, which are not relevant to us now. You don't see functions like `select()` or `mutate()` as they ran too quickly. So if you wanted to speed up `Fix_Data()` the place to look for improvements is in the date conversion. 
</details>


<details open><summary class='drop'> Benchmarking </summary>

Let's say that you wanted to speed up the `Fix_Data()` function. Benchmarking is a tool you can uses to compare different versions of a function to see which one runs faster. To do this we will use the `microbenchmark` package. Earlier the `as.POSIXct()` function was used to convert the `DateTime` columns from `character` to `date`. Lets see if that is quicker. We will also try the base function `strptime`.


```{r purr benchmark}

library(microbenchmark)

# Original function
Fix_Data_Lubridate <- function(data) {
  data %>%
    select(starts_with("Date") | starts_with("Temp") | starts_with("Intensity")) %>%
    rename("Date_Time" = starts_with("Date"), "Temp_F" = starts_with("Temp"), "Intensity" = starts_with("Intensity")) %>%
    mutate(Temp_C = 5 * (Temp_F - 32) / 9, Date_Time = mdy_hms(Date_Time), Date = date(Date_Time))
}


# New version using as.POSIXct()
Fix_Data_POSIXct <- function(data) {
  data %>%
    select(starts_with("Date") | starts_with("Temp") | starts_with("Intensity")) %>%
    rename("Date_Time" = starts_with("Date"), "Temp_F" = starts_with("Temp"), "Intensity" = starts_with("Intensity")) %>%
    mutate(
      Temp_C = 5 * (Temp_F - 32) / 9, Date_Time = as.POSIXct(Date_Time, "%m/%d/%y %H:%M:%S", tz = "UCT"),
      Date = date(Date_Time)
    )
}

# new version using strptime
Fix_Data_strptime <- function(data) {
  data %>%
    select(starts_with("Date") | starts_with("Temp") | starts_with("Intensity")) %>%
    rename("Date_Time" = starts_with("Date"), "Temp_F" = starts_with("Temp"), "Intensity" = starts_with("Intensity")) %>%
    mutate(
      Temp_C = 5 * (Temp_F - 32) / 9, Date_Time = strptime(Date_Time, "%m/%d/%y %H:%M:%S", tz = "UCT"),
      Date = date(Date_Time)
    )
}

# Do Comparison
mb<-microbenchmark(
   Lubridate = Fix_Data_Lubridate(intensity_data_raw[[2]]),
   POSIXct =  Fix_Data_POSIXct(intensity_data_raw[[2]]),
   Strptime = Fix_Data_strptime(intensity_data_raw[[2]]),
  times = 10L, unit = "ms"
)

mb

boxplot(mb)
```


As it happens, the `lubridate` version came out the fastest. You should also consider if this data is representative of the work you need to do. It is not unusual for one way of coding to be fastest with large amounts of data, but not have an advantage for small amounts of data. 


</details>
